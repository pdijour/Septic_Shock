{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ds4ph-bme/lab3-pdijour/blob/main/lab3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aB87u0EPLfWw"
      },
      "source": [
        "# Names (Enter your names below)\n",
        "**Your Name and JHED:** Phoebe Dijour, pdijour1\n",
        "\n",
        "**Partner's Name and JHED (If applicable):**  Kiana Bronder, kbronde1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUSjrn5zLfWx"
      },
      "source": [
        "# Lab 3: Prediction of Septic Shock in Patients\n",
        "\n",
        "By **Benjamín Béjar Haro** and edited by **Kwame Kutten** and **Joseph Greenstein**\n",
        "\n",
        "Sepsis is a life-threatening condition caused by an inflammatory immune response to an infection. It is the leading cause of death in hospitals and has a greater risk of mortality in its advanced state, also called *Septic Shock*. Early treatment of Septic Shock can dramatically increase the survival rate. Therefore, a prediction system capable of foreseeing Septic Shock would provide an early intervention window that has the potential to translate into improved patient outcomes. In this lab we look at the problem Septic Shock prediction following the approach described in [Liu et al. 2019](https://doi.org/10.1038/s41598-019-42637-5). Your goal in this lab is to reproduce some of the results in above paper. In particular you will train a a logistic regression model (referred to as GLM in the paper) to predict Septic Shock and will apply it to a test patient dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FObubMDyCtS5"
      },
      "outputs": [],
      "source": [
        "# Import modules\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import datetime as date\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Fix random number generator for reproducibility\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeaOItPULfW7"
      },
      "source": [
        "## Read Data\n",
        "You have been provided with the curated data used in Liu *et al.* which is a subset of the publicly available MIMIC-III database ([Johnson et al. 2016](https://doi.org/10.1038/sdata.2016.35)). The data corresponds to electronic health record data of a large population of patients, and consists of measured values over time for $28$ different features such as, heart rate, blood pressure, respiratory rate, temperature, etc. Each data point represents a particular measurement in time and for a particular patient. The data has been split into training and testing as described in Liu *et al.* and is provided to you in the form of `.csv` files. Inside those files `x` columns correspond to feature values while the `y` column represents the associated label of a particular row of feature values.  Thus $y=0$ means that the patient didn't go into Septic Shock, while a label $y=1$ indicates that the patient eventually went into Septic Shock)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5qjY8d9LfW7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e78243d-e7b7-43b7-f417-163618349a8c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-88165fc0d556>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Read training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtraindata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'glm.training.data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mXtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraindata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m29\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;31m# Rows are patients, columns are clinical indicators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mytrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraindata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './/glm.training.data.csv'"
          ]
        }
      ],
      "source": [
        "#===========================================\n",
        "# Read data. Change path if necessary\n",
        "#===========================================\n",
        "try:\n",
        "    # Executes if running in Google Colab\n",
        "    from google.colab import drive\n",
        "    drive.mount('gdrive/')\n",
        "    path = \"gdrive/My Drive/lab3_data/\" # Change path to location of data if necessary\n",
        "except:\n",
        "    # Executes if running locally (e.g. Anaconda)\n",
        "    path = \"./\"\n",
        "\n",
        "# Read training data\n",
        "traindata = pd.read_csv('/'.join((path,'glm.training.data.csv')))\n",
        "Xtrain = traindata.iloc[:,1:29].values # Rows are patients, columns are clinical indicators\n",
        "ytrain = traindata.iloc[:,-1].values\n",
        "\n",
        "# Read testing data\n",
        "testdata = pd.read_csv('/'.join((path,'glm.test.data.csv')))\n",
        "Xtest = testdata.iloc[:,1:29].values\n",
        "ytest = testdata.iloc[:,-1].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNPyondJLfW8"
      },
      "source": [
        "## 1. Normalize Data [5 points]\n",
        "Normalize the training and test data such that each column has zero mean and unit standard deviation.  Then for both the training and test data use `np.isclose` to verify that the means and standard deviations of the normalized data are correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdwZg2LzETyQ"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Write your code here\n",
        "\"\"\"\n",
        "# normalize variables\n",
        "# Xtest = (Xtest - np.mean(Xtest)) / np.std(Xtest)\n",
        "# Xtrain = (Xtrain - np.mean(Xtrain)) / np.std(Xtrain)\n",
        "# ytest = (ytest - np.mean(ytest)) / np.std(ytest)\n",
        "# ytrain = (ytrain - np.mean(ytrain)) / np.std(ytrain)\n",
        "Xtrain -= np.mean(Xtrain, axis=0)\n",
        "Xtrain /= np.std(Xtrain, axis=0)\n",
        "\n",
        "Xtest -= np.mean(Xtest, axis=0)\n",
        "Xtest /= np.std(Xtest, axis=0)\n",
        "\n",
        "# compare SDs\n",
        "print(np.isclose(np.std(Xtest), 1))\n",
        "print(np.isclose(np.std(Xtrain), 1))\n",
        "# print(np.isclose(np.std(ytest), 1))\n",
        "# print(np.isclose(np.std(ytrain), 1))\n",
        "# compare means\n",
        "print(np.isclose(np.mean(Xtest), 0))\n",
        "print(np.isclose(np.mean(Xtrain), 0))\n",
        "# print(np.isclose(np.mean(ytest), 0))\n",
        "# print(np.isclose(np.mean(ytrain), 0))\n",
        "\n",
        "# # convert to binary outcome\n",
        "# med_test = np.median(ytest)\n",
        "# ytest = (ytest > med_test) * 1\n",
        "# med_train = np.median(ytrain)\n",
        "# ytrain = (ytrain > med_train) * 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQoSg5ZOLfW8"
      },
      "source": [
        "## 2. Train Generalized Linear (Logistic Regression) Model [15 points total]\n",
        " * Train a logistic regression model on the normalized data using [Stochastic Gradient Descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent). You should use the [SGDClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) from `sklearn.linear_model` with a `\"log_loss\"` loss, `\"balanced\"` class weights and a `\"l1\"` lasso penalty.  Specify a value for regularization parameter $\\alpha \\in (0,1]$ [10 points]. \n",
        " * Plot the ROC curve and display the AUC [5 points]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hf-vBD2kLfW8"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Write your code here\n",
        "\"\"\"\n",
        "# create logistic model\n",
        "model = SGDClassifier(loss='log', penalty='l1', class_weight = 'balanced', alpha = 0.05)\n",
        "model.fit(Xtrain, ytrain)\n",
        "\n",
        "# calculate predicted y\n",
        "ypred = model.predict_proba(Xtrain)\n",
        "\n",
        "# calculate ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(ytrain, ypred[:,1])\n",
        "\n",
        "# Calculate AUC score\n",
        "auc_score = auc(fpr, tpr)\n",
        "\n",
        "# Calculate Operating Point\n",
        "max_operating_pt = 0\n",
        "for i in range(len(fpr)):\n",
        "  proportion = tpr[i]/fpr[i]\n",
        "  if proportion > max_operating_pt:\n",
        "    max_operating_pt = proportion\n",
        "    max_fpr = fpr[i]\n",
        "    max_tpr = tpr[i]\n",
        "\n",
        "# plot\n",
        "plt.figure()\n",
        "lw = 2\n",
        "plt.plot(fpr, tpr, color='darkorange',\n",
        "         lw=lw, label='ROC curve (area = %0.2f)' % auc_score)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "plt.scatter(max_fpr, max_tpr)\n",
        "plt.xlim([-0.05, 1.05])\n",
        "plt.ylim([-0.05, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Training Data Accuracy')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94IPLcmALfW9"
      },
      "source": [
        "## 3. Hyperparameter Tuning [20 points total] \n",
        " * Refine this model by determining an optimal regularization hyperparameter $\\alpha > 0$ which maximizes AUC via **5-fold cross validation** similar to Liu *et al.* You should use [KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) from `sklearn.model_selection` to split your training dataset into smaller chunks that you feed to the SGDClassifier. Try several values of $\\alpha \\in (0,1]$. To save time you may use a small number of iterations (e.g. 5) in this step for your SGDClassifier [15 points]. \n",
        " * Then display your optimal $\\alpha$ [5 points]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mA-8a8pELfW9"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Write your code here\n",
        "\"\"\"\n",
        "\n",
        "optimal_a_lst = []\n",
        "# Loop through the KFolds\n",
        "kf = KFold(n_splits = 5, shuffle = True)\n",
        "for train_idx, test_idx in kf.split(Xtrain):\n",
        "  X_train_f = Xtrain[train_idx, :]\n",
        "  y_train_f = ytrain[train_idx]\n",
        "  # Loop through different alphas\n",
        "  auc_score_max = 0\n",
        "  for a in np.linspace(0.01,0.1,5):\n",
        "    # Initialize SGD classifier\n",
        "    model = SGDClassifier(loss='log', penalty='l1', class_weight = 'balanced', alpha = a)\n",
        "    # Fit and predict\n",
        "    model.fit(X_train_f, y_train_f)\n",
        "    ypred_kf = model.predict_proba(Xtrain[test_idx, :])\n",
        "\n",
        "    # # Record ROC and AUC\n",
        "    fpr_kf, tpr_kf, thresholds = roc_curve(ytrain[test_idx], ypred_kf[:,1])\n",
        "    auc_score_kf = auc(fpr_kf, tpr_kf)\n",
        "    if auc_score_kf > auc_score_max:\n",
        "      print(auc_score_kf)\n",
        "      auc_score_max = auc_score_kf\n",
        "      optimal_a_ind = a\n",
        "  optimal_a_lst.append(optimal_a_ind)\n",
        "\n",
        "# Determine optimal alpha\n",
        "optimal_a = max(set(optimal_a_lst), key=optimal_a_lst.count)\n",
        "print(optimal_a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XbmAkbBLfW9"
      },
      "source": [
        "## 4. Retrain Model [20 points total]\n",
        " * Retrain your model using your optimal regularization hyperparameter $\\alpha$ from the previous step [5 points].\n",
        " * Plot the ROC curve and display it's AUC [5 points].\n",
        " * Find the **Operating Point** which gives the best TPR and FPR and add it to the plot [5 points].  \n",
        " * Display the training data accuracy at the operating point [5 points]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gF0RiPNELfW-"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Write your code here\n",
        "\"\"\"\n",
        "# Retrain model using optimal alpha\n",
        "# Plot the roc curve n display the auc\n",
        "# Find the operating point\n",
        "\n",
        "# create logistic model\n",
        "model = SGDClassifier(loss='log', penalty='l1', class_weight = 'balanced', alpha = optimal_a)\n",
        "model.fit(Xtrain, ytrain)\n",
        "\n",
        "# calculate predicted y\n",
        "ypred = model.predict_proba(Xtrain)\n",
        "\n",
        "# calculate ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(ytrain, ypred[:,1])\n",
        "\n",
        "# Calculate AUC score\n",
        "auc_score = auc(fpr, tpr)\n",
        "\n",
        "# Calculate Operating Point\n",
        "max_operating_pt = 0\n",
        "for i in range(len(fpr)):\n",
        "  proportion = tpr[i]/fpr[i]\n",
        "  if proportion > max_operating_pt:\n",
        "    max_operating_pt = proportion\n",
        "    max_fpr = fpr[i]\n",
        "    max_tpr = tpr[i]\n",
        "\n",
        "# Calculate training data accuracy\n",
        "accuracy = np.mean(ytrain == ypred[:,1].round())\n",
        "print(accuracy)\n",
        "\n",
        "# plot\n",
        "plt.figure()\n",
        "lw = 2\n",
        "plt.plot(fpr, tpr, color='darkorange',\n",
        "         lw=lw, label='ROC curve (area = %0.2f)' % auc_score)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "plt.scatter(max_fpr, max_tpr)\n",
        "plt.annotate('%0.2f'%accuracy, (max_fpr, max_tpr))\n",
        "plt.xlim([-0.05, 1.05])\n",
        "plt.ylim([-0.05, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Training Data Accuracy, alpha={}'.format(optimal_a))\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OY2ICFh6LfW-"
      },
      "source": [
        "## 5. Plot Feature Weights [10 points]\n",
        "The exponentiated coefficients from our model tell us how much each feature is weighted when making a prediction.  Find the weights by exponentiating the coefficients from your model (Get coefficients from the `'coef_` attribute). Plot a bar graph of these weights with their corresponding names.  Your results should be similar to Figure 3 in Liu *et al.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Hl1EMaQLfW-"
      },
      "outputs": [],
      "source": [
        "featureNames = traindata.iloc[:,1:29].keys() # Names of features\n",
        "\n",
        "\"\"\"\n",
        "Write your code here\n",
        "\"\"\"\n",
        "\n",
        "coeffs = np.exp(model.coef_[0])\n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.xticks(rotation = 45)\n",
        "plt.bar(featureNames, coeffs)\n",
        "plt.ylabel('Exponentiated Coefficient')\n",
        "plt.xlabel('Feature')\n",
        "plt.title('Feature Weights')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HVO6pgNLfW-"
      },
      "source": [
        "## 6. Test Model on Patient Data [30 points total]\n",
        " * Use the patient column from the test dataset to determine the number of patients in our test dataset.  Use the test labels to create an array of this size which is $1$ if the patient went into Septic Shock *at any time* during their hospital stay and $0$ otherwise.  We can be certain that a patient went into Septic Shock if their maximum probability (risk score) attained over their hospital stay excedes some operating threshold.  Therefore you should also create a corresponding array which contains these maximum probabilities [15 points].  \n",
        "\n",
        " * Create an ROC curve using these arrays and display the AUC [5 points].\n",
        " * Find operating point and add it to the plot [5 points].\n",
        " * Display the accuracy for test patients at the operating point [5 points]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NiBw8mWkLfW_"
      },
      "outputs": [],
      "source": [
        "patientsCol = testdata['patient'].values # Patients column from test dataset\n",
        "\n",
        "\"\"\"\n",
        "Write your code here\n",
        "\"\"\"\n",
        "# take all unique values from patient column\n",
        "# find indices of the unique values, put into a list\n",
        "# probability = predict_proba(x_test) at those indices\n",
        "# ytest at those indices\n",
        "# Get ROC and AUC from ytest and probability\n",
        "\n",
        "patients_idx = []\n",
        "patients = np.unique(patientsCol)\n",
        "# for i in patients:\n",
        "#   # patients_idx.append(np.where(patientsCol == i))\n",
        "#   patients_idx.append(patientsCol.tolist().index(i))\n",
        "\n",
        "ytest_pt = []\n",
        "ypred_max = []\n",
        "\n",
        "ypred = model.predict_proba(Xtest)[:,1]\n",
        "\n",
        "for i in patients:\n",
        "  # find time indices for each unique patient\n",
        "  indices = np.where(patientsCol == i)\n",
        "  # for each pt, find if they went into septic shock during hospital stay\n",
        "  patient = ytest[indices]\n",
        "  if patient.max() == 1:\n",
        "    ytest_pt.append(1)\n",
        "  else:\n",
        "    ytest_pt.append(0)\n",
        "  \n",
        "  # for each pt, find max risk score \n",
        "  ypred_idx = ypred[indices]\n",
        "  ypred_max.append(ypred_idx.max())\n",
        "\n",
        "# calculate ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(ytest_pt, ypred_max)\n",
        "\n",
        "# Calculate AUC score\n",
        "auc_score = auc(fpr, tpr)\n",
        "\n",
        "# Calculate Operating Point\n",
        "max_operating_pt = 0\n",
        "for i in range(len(fpr)):\n",
        "  proportion = tpr[i]/fpr[i]\n",
        "  if proportion > max_operating_pt:\n",
        "    max_operating_pt = proportion\n",
        "    max_fpr = fpr[i]\n",
        "    max_tpr = tpr[i]\n",
        "\n",
        "# Calculate testing data accuracy\n",
        "accuracy_test = np.mean(ytest_pt == np.array(ypred_max).round())\n",
        "print(accuracy_test)\n",
        "\n",
        "# plot\n",
        "plt.figure()\n",
        "lw = 2\n",
        "plt.plot(fpr, tpr, color='darkorange',\n",
        "         lw=lw, label='ROC curve (area = %0.2f)' % auc_score)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "plt.scatter(max_fpr, max_tpr)\n",
        "plt.annotate('%0.2f'%accuracy_test, (max_fpr, max_tpr))\n",
        "plt.xlim([-0.05, 1.05])\n",
        "plt.ylim([-0.05, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Testing Data Accuracy on Real Patient Data')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}